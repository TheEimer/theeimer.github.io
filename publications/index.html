<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Theresa Eimer</title> <meta name="author" content="Theresa Eimer"> <meta name="description" content="Publications are listed in reversed chronological order."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%8D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://theeimer.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Theresa </span>Eimer</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Hi there!</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item active"> <a class="nav-link text-lowercase" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/teaching/">Teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">Publications are listed in reversed chronological order.</p> </header> <article> <div class="publications"> <nav id="year-nav" class="navbar fixed-bottom container" style="margin-bottom: -50px; align-self: center;"> <p class="post-description" style="padding-bottom: 15px; align-self: center"> Jump to: <a href="#year-2023" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2023</a> <a href="#year-2022" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2022</a> <a href="#year-2021" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2021</a> <a href="#year-2020" class="btn btn-sm z-depth-0" style="padding: 0 0 0 0" role="button">2020</a> </p> </nav> <h2 class="year" id="year-2023">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="eimer-icml23" class="col-sm-8"> <div class="title">Hyperparameters in Reinforcement Learning and How To Tune Them</div> <div class="author"> <em>Theresa Eimer</em>, <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a>, and <a href="https://rraileanu.github.io/" rel="external nofollow noopener" target="_blank">Roberta Raileanu</a> </div> <div class="periodical"> <em>Proceedings of the Fortieth International Conference on Machine Learning</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/icml_23_hpo_in_rl.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/facebookresearch/how-to-autorl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://sites.google.com/auto-ai.org/hpo-in-rl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In order to improve reproducibility, deep reinforcement learning (RL) has been adopting better scientific practices such as standardized evaluation metrics and reporting. However, the process of hyperparameter optimization still varies widely across papers, which makes it challenging to compare RL algorithms fairly. In this paper, we show that hyperparameter choices in RL can significantly affect the agent’s final performance and sample efficiency, and that the hyperparameter landscape can strongly depend on the tuning seed which may lead to overfitting. We therefore propose adopting established best practices from AutoML, such as the separation of tuning and testing seeds, as well as principled hyperparameter optimization (HPO) across a broad search space. We support this by comparing multiple state-of-the-art HPO tools on a range of RL algorithms and environments to their hand-tuned counterparts, demonstrating that HPO approaches often have higher performance and lower compute overhead. As a result of our findings, we recommend a set of best practices for the RL community, which should result in stronger empirical results with fewer computational costs, better reproducibility, and thus faster progress. In order to encourage the adoption of these practices, we provide plug-and-play implementations of the tuning algorithms used in this paper. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">eimer-icml23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Lindauer, Marius and Raileanu, Roberta}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hyperparameters in Reinforcement Learning and How To Tune Them}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings of the Fortieth International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="benjamins-tmlr23" class="col-sm-8"> <div class="title">Contextualize Me - The Case for Context in Reinforcement Learning</div> <div class="author"> <a href="https://www.ai.uni-hannover.de/en/institut/team/benjamins" rel="external nofollow noopener" target="_blank">Carolin Benjamins</a>, <em>Theresa Eimer</em>, <a href="http://www.tnt.uni-hannover.de/en/staff/schubert/" rel="external nofollow noopener" target="_blank">Frederik Schubert</a>, Sebastian Döhler, <a href="https://www.ai.uni-hannover.de/en/institut/team/mohan" rel="external nofollow noopener" target="_blank">Aditya Mohan</a>, <a href="https://andrebiedenkapp.github.io/" rel="external nofollow noopener" target="_blank">André Biedenkapp</a>, <a href="http://www.tnt.uni-hannover.de/en/staff/rosenhahn/" rel="external nofollow noopener" target="_blank">Bodo Rosenhahn</a>, <a href="https://ml.informatik.uni-freiburg.de/profile/hutter" rel="external nofollow noopener" target="_blank">Frank Hutter</a>, and <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a> </div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/tmlr_23_contextualize_me.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/automl/CARL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.automl.org/carl-a-benchmark-to-study-generalization-in-reinforcement-learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>While Reinforcement Learning (RL) has made great strides towards solving increasingly complicated problems, many algorithms are still brittle to even slight environmental changes. Contextual Reinforcement Learning (cRL) provides a framework to model such changes in a principled manner, thereby enabling flexible, precise and interpretable task specification and generation. Our goal is to show how the framework of cRL contributes to improving zero-shot generalization in RL through meaningful benchmarks and structured reasoning about generalization tasks. We confirm the insight that optimal behavior in cRL requires context information, as in other related areas of partial observability. To empirically validate this in the cRL framework, we provide various context-extended versions of common RL environments. They are part of the first benchmark library, CARL, designed for generalization based on cRL extensions of popular benchmarks, which we propose as a testbed to further study general agents. We show that in the contextual setting, even simple RL environments become challenging - and that naive solutions are not enough to generalize across complex context spaces.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">benjamins-tmlr23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Benjamins, Carolin and Eimer, Theresa and Schubert, Frederik and Döhler, Sebastian and Mohan, Aditya and Biedenkapp, André and Rosenhahn, Bodo and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Contextualize Me - The Case for Context in Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2022">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="parker-holder-jair22" class="col-sm-8"> <div class="title">Automated Reinforcement Learning (AutoRL): A Survey and Open Problems</div> <div class="author"> Jack Parker-Holder, <a href="https://ml.informatik.uni-freiburg.de/profile/rajan/" rel="external nofollow noopener" target="_blank">Raghu Rajan</a>, Xingyou Song, <a href="https://andrebiedenkapp.github.io/" rel="external nofollow noopener" target="_blank">André Biedenkapp</a>, Yingjie Miao, <em>Theresa Eimer</em>, Baohe Zhang, Vu Nguyen, Roberto Calandra, Aleksandra Faust, <a href="https://ml.informatik.uni-freiburg.de/profile/hutter" rel="external nofollow noopener" target="_blank">Frank Hutter</a>, and <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a> </div> <div class="periodical"> <em>Journal of Artificial Intelligence Research (JAIR)</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/jair_22_autorl_survey.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.automl.org/automated-reinforcement-learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The combination of Reinforcement Learning (RL) with deep learning has led to a series of impressive feats, with many believing (deep) RL provides a path towards generally capable agents. However, the success of RL agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. This makes it challenging to use RL for new problems, while also limits its full potential. In many other areas of machine learning, AutoML has shown it is possible to automate such design choices and has also yielded promising initial results when applied to RL. However, Automated Reinforcement Learning (AutoRL) involves not only standard applications of AutoML but also includes additional challenges unique to RL, that naturally produce a different set of methods. As such, AutoRL has been emerging as an important area of research in RL, providing promise in a variety of applications from RNA design to playing games such as Go. Given the diversity of methods and environments considered in RL, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. In this survey we seek to unify the field of AutoRL, we provide a common taxonomy, discuss each area in detail and pose open problems which would be of interest to researchers going forward.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">parker-holder-jair22</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Parker{-}Holder, Jack and Rajan, Raghu and Song, Xingyou and Biedenkapp, André and Miao, Yingjie and Eimer, Theresa and Zhang, Baohe and Nguyen, Vu and Calandra, Roberto and Faust, Aleksandra and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Reinforcement Learning (AutoRL): {A} Survey and Open Problems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Artificial Intelligence Research (JAIR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{517-568}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{74}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="adriaensen-jair22" class="col-sm-8"> <div class="title">Automated Dynamic Algorithm Configuration</div> <div class="author"> <a href="https://ml.informatik.uni-freiburg.de/profile/adriaensen/" rel="external nofollow noopener" target="_blank">Steven Adriaensen</a>, <a href="https://andrebiedenkapp.github.io/" rel="external nofollow noopener" target="_blank">André Biedenkapp</a>, Gresa Shala, Noor Awad, <em>Theresa Eimer</em>, <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a>, and <a href="https://ml.informatik.uni-freiburg.de/profile/hutter" rel="external nofollow noopener" target="_blank">Frank Hutter</a> </div> <div class="periodical"> <em>Journal of Artificial Intelligence Research</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/jair_22_dac.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/automl/2022_JAIR_DAC_experiments" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.automl.org/automated-algorithm-design/dac/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The performance of an algorithm often critically depends on its parameter configuration. While a variety of automated algorithm configuration methods have been proposed to relieve users from the tedious and error-prone task of manually tuning parameters, there is still a lot of untapped potential as the learned configuration is static, i.e., parameter settings remain fixed throughout the run. However, it has been shown that some algorithm parameters are best adjusted dynamically during execution, e.g., to adapt to the current part of the optimization landscape. Thus far, this is most commonly achieved through hand-crafted heuristics. A promising recent alternative is to automatically learn such dynamic parameter adaptation policies from data. In this article, we give the first comprehensive account of this new field of automated dynamic algorithm configuration (DAC), present a series of recent advances, and provide a solid foundation for future research in this field. Specifically, we (i) situate DAC in the broader historical context of AI research; (ii) formalize DAC as a computational problem; (iii) identify the methods used in prior-art to tackle this problem; (iv) conduct empirical case studies for using DAC in evolutionary optimization, AI planning, and machine learning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">adriaensen-jair22</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Adriaensen, Steven and Biedenkapp, André and Shala, Gresa and Awad, Noor and Eimer, Theresa and Lindauer, Marius and Hutter, Frank}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automated Dynamic Algorithm Configuration}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Artificial Intelligence Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{75}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1633--1699}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="eimer-ecorl25" class="col-sm-8"> <div class="title">Test</div> <div class="author"> <em>Theresa Eimer</em>, <a href="https://www.ai.uni-hannover.de/en/institut/team/benjamins" rel="external nofollow noopener" target="_blank">Carolin Benjamins</a>, and <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a> </div> <div class="periodical"> <em>In Ecological Theory of RL Workshop NeurIPS</em>, Dez 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/ecorl_21_crl_hpo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Although Reinforcement Learning (RL) has shown impressive results in games and simulation, real-world application of RL suffers from its instability under changing environment conditions and hyperparameters. We give a first impression of the extent of this instability by showing that the hyperparameters found by automatic hyperparameter optimization (HPO) methods are not only dependent on the problem at hand, but even on how well the state describes the environment dynamics. Specifically, we show that agents in contextual RL require different hyperparameters if they are shown how environmental factors change. In addition, finding adequate hyperparameter configurations is not equally easy for both settings, further highlighting the need for research into how hyperparameters influence learning and generalization in RL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eimer-ecorl25</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Benjamins, Carolin and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Test}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Ecological Theory of RL Workshop NeurIPS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dez</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2021">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="eimer-ijcai21" class="col-sm-8"> <div class="title">DACBench: A Benchmark Library for Dynamic Algorithm Configuration</div> <div class="author"> <em>Theresa Eimer</em>, <a href="https://andrebiedenkapp.github.io/" rel="external nofollow noopener" target="_blank">André Biedenkapp</a>, Maximilian Reimer, <a href="https://ml.informatik.uni-freiburg.de/profile/adriaensen/" rel="external nofollow noopener" target="_blank">Steven Adriaensen</a>, <a href="https://ml.informatik.uni-freiburg.de/profile/hutter" rel="external nofollow noopener" target="_blank">Frank Hutter</a>, and <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI’21)</em>, Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/ijcai_21_dacbench.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.automl.org/dacbench-benchmarking-dynamic-algorithm-configuration/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/automl/DACBench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.automl.org/automated-algorithm-design/dac/dacbench-benchmarking-dynamic-algorithm-configuration/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Dynamic Algorithm Configuration (DAC) aims to dynamically control a target algorithm’s hyperparameters in order to improve its performance. Several theoretical and empirical results have demonstrated the benefits of dynamically controlling hyperparameters in domains like evolutionary computation, AI Planning or deep learning. Replicating these results, as well as studying new methods for DAC, however, is difficult since existing benchmarks are often specialized and incompatible with the same interfaces. To facilitate benchmarking and thus research on DAC, we propose DACBench, a benchmark library that seeks to collect and standardize existing DAC benchmarks from different AI domains, as well as provide a template for new ones. For the design of DACBench, we focused on important desiderata, such as (i) flexibility, (ii) reproducibility, (iii) extensibility and (iv) automatic documentation and visualization. To show the potential, broad applicability and challenges of DAC, we explore how a set of six initial benchmarks compare in several dimensions of difficulty. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eimer-ijcai21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Biedenkapp, André and Reimer, Maximilian and Adriaensen, Steven and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DACBench: A Benchmark Library for Dynamic Algorithm Configuration}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirtieth International Joint Conference on
                 Artificial Intelligence ({IJCAI}'21)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ijcai.org}</span><span class="p">,</span>
  <span class="na">talk</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=-G-hLmBI4WM}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="eimer-icml21" class="col-sm-8"> <div class="title">Self-Paced Context Evaluation for Contextual Reinforcement Learning</div> <div class="author"> <em>Theresa Eimer</em>, <a href="https://andrebiedenkapp.github.io/" rel="external nofollow noopener" target="_blank">André Biedenkapp</a>, <a href="https://ml.informatik.uni-freiburg.de/profile/hutter" rel="external nofollow noopener" target="_blank">Frank Hutter</a>, and <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the Thirty-eighth International Conference on Machine Learning</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/icml_21_space.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.automl.org/self-paced-context-evaluation-for-contextual-reinforcement-learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/automl/SPaCE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) has made a lot of advances for solving a single problem in a given environment; but learning policies that generalize to unseen variations of a problem remains challenging. To improve sample efficiency for learning on such instances of a problem domain, we present Self-Paced Context Evaluation (SPaCE). Based on self-paced learning, \spc automatically generates \task curricula online with little computational overhead. To this end, SPaCE leverages information contained in state values during training to accelerate and improve training performance as well as generalization capabilities to new instances from the same problem domain. Nevertheless, SPaCE is independent of the problem domain at hand and can be applied on top of any RL agent with state-value function approximation. We demonstrate SPaCE’s ability to speed up learning of different value-based RL agents on two environments, showing better generalization capabilities and up to 10x faster learning compared to naive approaches such as round robin or SPDRL, as the closest state-of-the-art approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eimer-icml21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Biedenkapp, André and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Self-Paced Context Evaluation for Contextual Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirty-eighth International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="schubert-rl4rlicml21" class="col-sm-8"> <div class="title">Automatic Risk Adaption in Distributional Reinforcement Learning</div> <div class="author"> <a href="http://www.tnt.uni-hannover.de/en/staff/schubert/" rel="external nofollow noopener" target="_blank">Frederik Schubert</a>, <em>Theresa Eimer</em>, <a href="http://www.tnt.uni-hannover.de/en/staff/rosenhahn/" rel="external nofollow noopener" target="_blank">Bodo Rosenhahn</a>, and <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a> </div> <div class="periodical"> <em>In Workshop on Reinforcement Learning for Real Life (RL4RealLife@ICML’21)</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/rl4rl_21_ara.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The use of Reinforcement Learning (RL) agents in practical applications requires the consideration of suboptimal outcomes, depending on the familiarity of the agent with its environment. This is especially important in safety-critical environments, where errors can lead to high costs or damage. In distributional RL, the risk-sensitivity can be controlled via different distortion measures of the estimated return distribution. However, these distortion functions require an estimate of the risk level, which is difficult to obtain and depends on the current state. In this work, we demonstrate the suboptimality of a static risk level estimation and propose a method to dynamically select risk levels at each environment step. Our method ARA (Automatic Risk Adaptation) estimates the appropriate risk level in both known and unknown environments using a Random Network Distillation error. We show reduced failure rates by up to a factor of 7 and improved generalization performance by up to 14 percent compared to both risk-aware and risk-agnostic agents in several locomotion environments.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">schubert-rl4rlicml21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Schubert, Frederik and Eimer, Theresa and Rosenhahn, Bodo and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic Risk Adaption in Distributional Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Reinforcement Learning for Real Life ({RL4RealLife@ICML}'21)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="eimer-ecorl21" class="col-sm-8"> <div class="title">Hyperparameters in Contextual RL are Highly Situational</div> <div class="author"> <em>Theresa Eimer</em>, <a href="https://www.ai.uni-hannover.de/en/institut/team/benjamins" rel="external nofollow noopener" target="_blank">Carolin Benjamins</a>, and <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a> </div> <div class="periodical"> <em>In Ecological Theory of RL Workshop NeurIPS</em>, Dez 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/ecorl_21_crl_hpo.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p> Although Reinforcement Learning (RL) has shown impressive results in games and simulation, real-world application of RL suffers from its instability under changing environment conditions and hyperparameters. We give a first impression of the extent of this instability by showing that the hyperparameters found by automatic hyperparameter optimization (HPO) methods are not only dependent on the problem at hand, but even on how well the state describes the environment dynamics. Specifically, we show that agents in contextual RL require different hyperparameters if they are shown how environmental factors change. In addition, finding adequate hyperparameter configurations is not equally easy for both settings, further highlighting the need for research into how hyperparameters influence learning and generalization in RL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">eimer-ecorl21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Eimer, Theresa and Benjamins, Carolin and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hyperparameters in Contextual RL are Highly Situational}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Ecological Theory of RL Workshop NeurIPS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dez</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year" id="year-2020">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="biedenkapp-ecai20" class="col-sm-8"> <div class="title">Dynamic Algorithm Configuration: Foundation of a New Meta-Algorithmic Framework</div> <div class="author"> <a href="https://andrebiedenkapp.github.io/" rel="external nofollow noopener" target="_blank">André Biedenkapp</a>, H. Furkan Bozkurt, <em>Theresa Eimer</em>, <a href="https://ml.informatik.uni-freiburg.de/profile/hutter" rel="external nofollow noopener" target="_blank">Frank Hutter</a>, and <a href="https://www.ai.uni-hannover.de/en/institut/team/lindauer" rel="external nofollow noopener" target="_blank">Marius Lindauer</a> </div> <div class="periodical"> <em>In Proceedings of the European Conference on Artificial Intelligence (ECAI)</em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/paper/ecai_20_dac.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.automl.org/dynamic-algorithm-configuration/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/automl/DAC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.automl.org/automated-algorithm-design/dac/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The performance of many algorithms in the fields of hard combinatorial problem solving, machine learning or AI in general depends on parameter tuning. Automated methods have been proposed to alleviate users from the tedious and error-prone task of manually searching for performance-optimized configurations across a set of problem instances. However, there is still a lot of untapped potential through adjusting an algorithm’s parameters online since different parameter values can be optimal at different stages of the algorithm. Prior work showed that reinforcement learning is an effective approach to learn policies for online adjustments of algorithm parameters in a data-driven way. We extend that approach by formulating the resulting dynamic algorithm configuration as a contextual MDP, such that RL not only learns a policy for a single instance, but across a set of instances. To lay the foundation for studying dynamic algorithm configuration with RL in a controlled setting, we propose white-box benchmarks covering major aspects that make dynamic algorithm configuration a hard problem in practice and study the per- formance of various types of configuration strategies for them. On these white-box benchmarks, we show that (i) RL is a robust candidate for learning configuration policies, outperforming standard pa- rameter optimization approaches, such as classical algorithm configuration; (ii) based on function approximation, RL agents can learn to generalize to new types of instances; and (iii) self-paced learning can substantially improve the performance by selecting a useful sequence of training instances automatically.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">biedenkapp-ecai20</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Biedenkapp, André and Bozkurt, H. Furkan and Eimer, Theresa and Hutter, Frank and Lindauer, Marius}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic {A}lgorithm {C}onfiguration: {F}oundation of a {N}ew {M}eta-{A}lgorithmic {F}ramework}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the European Conference on Artificial Intelligence (ECAI)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">talk</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=wxPYtSGT05s&amp;feature=youtu.be}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Theresa Eimer. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. <a href="https://theeimer.github.io/impressum/">Impressum</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>