<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://theeimer.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://theeimer.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-22T11:17:32+00:00</updated><id>https://theeimer.github.io/feed.xml</id><title type="html">blank</title><subtitle>Hi, I&apos;m Theresa, a RL researcher on the hunt for general agents and DACs. </subtitle><entry><title type="html">EWRL17 Recap</title><link href="https://theeimer.github.io/blog/2024/ewrl/" rel="alternate" type="text/html" title="EWRL17 Recap"/><published>2024-11-03T15:59:00+00:00</published><updated>2024-11-03T15:59:00+00:00</updated><id>https://theeimer.github.io/blog/2024/ewrl</id><content type="html" xml:base="https://theeimer.github.io/blog/2024/ewrl/"><![CDATA[<h1 id="ewrl17-recap">EWRL17 Recap</h1> <p>This year’s EWRL in Toulouse was a great event: well-organized, interesting program, awesome people. I can only recommend attending the next edition! Here are the most interesting papers I discovered during the poster sessions - if yours isn’t here, it’s most likely due to me getting too caught up chatting or the wine during lunch. I ordered the papers according to the EWRL website, so you shouldn’t take that as an indicator of quality in any way.</p> <h3 id="deterministic-exploration-via-stationary-bellman-error-maximization"><a href="https://openreview.net/forum?id=vYm3GOD8CG">Deterministic Exploration via Stationary Bellman Error Maximization</a></h3> <p>The idea here is to use an exploration strategy that is trained to find really high error states of the actual policy. The fun thing: this policy apparently can discover quite complex Lunar Lander behaviors (e.g. barrel rolls!) this way. It struck me how much more complex the error behavior was compared to the actual solution, I’d be interested to know if it’s actually easier to find complex errors than to do the right thing.</p> <h3 id="maximum-entropy-on-policy-actor-critic-via-entropy-advantage-estimation"><a href="https://openreview.net/forum?id=4osMPTZFBF">Maximum Entropy On-Policy Actor-Critic via Entropy Advantage Estimation</a></h3> <p>The interesting bit here is that we should probably treat the entropy and value advantage estimation differently - if put like that, it seems obvious and the results here support why. Depending on the environment, we want more or less exploration decay and using this method we can control that separately. Basically, we expose a hyperparameter that so far was kept completely static and thus had to serve the same purpose for advantage and entropy estimation. Being able to adjust it is likely going to be an advantage, especially if we can figure out how to do that on the fly.</p> <h3 id="interpretable-and-editable-programmatic-tree-policies-for-reinforcement-learning"><a href="https://openreview.net/forum?id=yDicN3WVZ2">Interpretable and Editable Programmatic Tree Policies for Reinforcement Learning</a></h3> <p>This sounds like such a crazy idea: let’s translate RL policies into Python code! It’s even crazier that the authors figured out how to do this. I’m impressed and really want to try this now.</p> <h3 id="explore-go-leveraging-exploration-for-generalisation-in-deep-reinforcement-learning"><a href="https://openreview.net/forum?id=qteUVvGvFQ">Explore-Go: Leveraging Exploration for Generalisation in Deep Reinforcement Learning</a></h3> <p>Yes, the title refers to Go-Explore, so I asked the authors. This is a much simpler idea, though: just by randomly finding new “starting states” at the beginning of an episode, we can improve generalization. Obviously, there are environments where random actions aren’t helpful, but the point stands: artificially increasing diversity in starting states is important for learning. The fact that we can accomplish it so easily is pretty neat!</p> <h3 id="revisiting-on-policy-deep-reinforcement-learning"><a href="https://openreview.net/forum?id=SYV6AlWh9P">Revisiting On-Policy Deep Reinforcement Learning</a></h3> <p>This paper ablates a few interesting design decisions in SAC and compares it to PPO. What I find most interesting (quite obviously if you know me) is of course the fact that some of these design decisions seem to be somewhat environment-agnostic, at least on MuJoCo, while others might be interesting to adapt, e.g. Momentum or Off-Policy. The difference is also only really visible on the Inverted Double Pendulum, kind of the odd one out environment - meaning that their ON-SAC configuration seems pretty stable within the MuJoCo locomotion domain, but might need adjustment elsewhere. Interesting signals when thinking about connecting algorithm configurations to environment domains.</p> <h3 id="image-based-dataset-representations-for-predicting-learning-performance-in-offline-rl"><a href="https://openreview.net/forum?id=AKU4h6BPG7">Image-Based Dataset Representations for Predicting Learning Performance in Offline RL</a></h3> <p>Another crazy-sounding idea: let’s compress an offline RL dataset into an image for performance prediction! This is a really creative paper and a great idea overall, I recommend taking a look.</p> <h3 id="curricula-for-learning-robust-policies-with-factored-state-representations-in-changing-environments"><a href="https://openreview.net/forum?id=X3i12AKYJn">Curricula for Learning Robust Policies with Factored State Representations in Changing Environments</a></h3> <p>This paper contains quite a few interesting ablations centered around the question of how well an agent generalizes on Frozen Lake variations. It reinforces that even here in this simple environment we don’t really know how to do task scheduling, that different context features have different influences on generalization and that the state representation matters. The ablations are quite thorough and on a simple domain, so I think they’re a nice reference on different factors for zero-shot generalization and task ordering.</p> <h3 id="value-improved-actor-critic-algorithms"><a href="https://openreview.net/forum?id=rXSgsdvpV9">Value Improved Actor Critic Algorithms</a></h3> <p>Why do we use policy improvement operators and not value improvement operators (explicit ones, at least)? This paper tries and there are a few interesting results - most importantly maybe that value improvement operators allow more (or less, if you prefer) greedy improvement than before. This interacts nicely with over- vs underestimation: the danger of being “too pessimistic” in the long run is reduced if we’re very greedy with respect to the pessimistic estimate, which is better than trying to force a more optimistic estimate in my opinion.</p> <h3 id="trust-the-model-where-it-trusts-itself---model-based-actor-critic-with-uncertainty-aware-rollout-adaption"><a href="https://openreview.net/forum?id=VRJaXWiu7j">Trust the Model Where It Trusts Itself - Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption</a></h3> <p>I was almost convinced this method must have existed beforehand when I came to the poster - this is a compliment! Using the model uncertainty to gauge whether it’s better to use the model to generate samples or collect new experiences feels so much more natural than scheduling the tradeoff without taking the model into account.</p> <h3 id="autorlorg-papers">AutoRL.org Papers</h3> <p>I left out papers I’d seen previously and that have been presented by the AutoRL.org circle - you should still take a look at those, though: <a href="https://openreview.net/forum?id=ZEnbCWsxoL">ARLBench</a>, <a href="https://openreview.net/forum?id=x60cH4KDRv">Relational Structure in Representations</a>, <a href="https://openreview.net/forum?id=LkYaQzGgfL">Joint Optimization of Reward Shaping and Hyperparameters</a> and the wonderfully titled <a href="https://openreview.net/forum?id=zHt4K5zX4P">Dreaming of Many Worlds</a>.</p> <h3 id="bonus">Bonus</h3> <p>I really liked Elise van der Pol’s talk about symmetry. I was vaguely aware of the concepts she talked about before, but apparently it took until now to really connect with me. Super interesting topic and I hope to see more research here!</p> <p>Thanks for reading, hopefully see you again next year!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The EWRL in Toulouse was an awesome event. Here are some of my favorite posters!]]></summary></entry><entry><title type="html">Introducing Hypersweeper: Bridging the HPO Gap Between AutoML Research and ML Practitioners</title><link href="https://theeimer.github.io/blog/2024/introducing-hypersweeper-bridging-the-hpo-gap-between-automl-research-and-ml-practitioners/" rel="alternate" type="text/html" title="Introducing Hypersweeper: Bridging the HPO Gap Between AutoML Research and ML Practitioners"/><published>2024-09-25T09:39:01+00:00</published><updated>2024-09-25T09:39:01+00:00</updated><id>https://theeimer.github.io/blog/2024/introducing-hypersweeper-bridging-the-hpo-gap-between-automl-research-and-ml-practitioners</id><content type="html" xml:base="https://theeimer.github.io/blog/2024/introducing-hypersweeper-bridging-the-hpo-gap-between-automl-research-and-ml-practitioners/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[The lack of widespread adoption of AutoML tools in the broader ML community has been a recurring topic of discussion within the field. Is this due to a lack of trust in these systems? Do our benchmarks fail to reflect real-world use cases? Or is it simply too difficult to find and implement state-of-the-art methods? [&#8230;]]]></summary></entry><entry><title type="html">2023 in AutoRL</title><link href="https://theeimer.github.io/blog/2024/AutoRL-redirect/" rel="alternate" type="text/html" title="2023 in AutoRL"/><published>2024-01-11T10:00:00+00:00</published><updated>2024-01-11T10:00:00+00:00</updated><id>https://theeimer.github.io/blog/2024/AutoRL-redirect</id><content type="html" xml:base="https://theeimer.github.io/blog/2024/AutoRL-redirect/"><![CDATA[<p>Let’s test something here</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A retrospective on 2023 in AutoRL research written by a collective of AutoRL researchers. Redirects to the post on the AutoRL blog.]]></summary></entry><entry><title type="html">Why You Should Try Science Communication During Your PhD</title><link href="https://theeimer.github.io/blog/2023/scicomm/" rel="alternate" type="text/html" title="Why You Should Try Science Communication During Your PhD"/><published>2023-10-04T15:59:00+00:00</published><updated>2023-10-04T15:59:00+00:00</updated><id>https://theeimer.github.io/blog/2023/scicomm</id><content type="html" xml:base="https://theeimer.github.io/blog/2023/scicomm/"><![CDATA[<h2 id="why-you-should-try-science-communication-during-your-phd">Why You Should Try Science Communication During Your PhD</h2> <p>Communicating our work is a big part of being a researcher - writing papers, presenting posters, giving talks, even just office brainstorming is communication after all. But often it’s quite limited in the sense that normally we aim to talk to people that are well versed in our topic or at least our field. As an example, in RL papers or talks few people explain the details of the standard RL problem formulation and how the algorithm they use as a baseline tries to solve it, they instead talk about the parts that are relevant for their approach. Maybe they use a different setting than the standard MDP or develop an extension for DQN and thus to help people understand that novel aspect, they recap the connection points to the literature. And that’s good, I think trying to do anything else in a conference or 15 minute presentation will waste a bunch of people’s time, in such a setting we expect people to have a basic understanding of common concepts. I just think that it can be a really good experience to get out of these settings and talk to a broader range of people. Take a look at this (very professional and very accurate) Venn diagram I drew up: How many of the categories of people do you tell about your research (Let’s not count Mum, Dad and Grandma here)?</p> <p><img src="https://theeimer.github.io/assets/img/scicomm_venn.png" alt="Venn diagram: research community in research field in large &quot;everyone&quot; circle"/></p> <p>I talk to people working on the same topic as me a few times a week, update a wider circle every few weeks informally, give internal presentations multiple times per year and then have a handful of conferences and/or workshops in there. Maybe one or two occasions per year will have me explain what I do to someone not working on ML topics directly but at least invested in computer science in some way. That’s already quite a bit, I think, and it’s because my group has solid workflows around presentation and also because I tend to enjoy talking research. Until this year, however, I didn’t really have many opportunities to speak outside of research settings and after I did, I noticed it’s definitely something else. It’s hard, time-consuming and has little direct payoff. I really like it, though, and think you should try as well, even (or maybe especially?) during your already stressful PhD. In this blog post I want to outline why I think that and give you some ideas for things to try that shouldn’t take too much effort and are pretty much free.</p> <h3 id="what-is-science-communication">What is Science Communication?</h3> <p>Wikipedia says “Science communication encompasses a wide range of activities that connect science and society. Common goals of science communication include informing non-experts about scientific findings, raising the public awareness of and interest in science, influencing people’s attitudes and behaviors, informing public policy, and engaging with diverse communities to address societal problems.” - and actually a lot more than that. What’s important though: we’re not talking to experts, we’re telling everyone else what we’re researching.</p> <p>That can come in many forms, you could show a robot off to third graders, give a presentation at a business conference, write a newspaper article or even take part in a podium discussion at a public event, all of that is Science Communication. If you look for #SciComm on social media, you’ll find tons more examples. And obviously it’s important to keep non-experts updated with research, climate change is a great example here. <em>But that’s very large scale and should be done by professors, science journalists or even scientific organizations. It’s not a field for PhD students, right?</em></p> <h3 id="why-should-you-actively-look-for-scicomm-opportunities">Why should you actively look for SciComm opportunities?</h3> <p>There’s two ways to understand this question. The first is <em>“What can other people take away from my research, I’m just a PhD student?”</em>. And I think the answer is “more than you think”. Yes, most PhD students work on super specific questions - but you’re doing it because you think I’ll make a difference in the grand scheme of things, right? I think if done well it might actually be more interesting to hear about one specific idea than get an overview of a whole field simply because the whole field will be much harder to grasp, especially without a lot of background knowledge. Getting deep into one detail, though, is totally possible for the audience if you break it down far enough and then they might feel like they actually understood the core of it. I know I’d certainly prefer to learn about one specific detail of, let’s say carpentry, than someone trying to give me a rough overview of all techniques – that would just leave me confused, most likely.</p> <p>The other implication of the question is <em>“But why do I have to be the one, can’t someone else do it? What’s in it for me?”</em>. To a degree that’s true, you won’t be rewarded for giving a general public presentation in the same way you would be for giving an academic one (in terms of new connections, feedback, exposure of your work…), plus it’s a lot more work. But I think making your topic understandable for anyone is a great skill to have. It’s a lot about storytelling, finding everyday analogies, about trimming down the details until you’re left with the core idea of your work. I never had to do this before and the first time it was hard and took a lot of time. Then it got easier. And I notice how it’s useful in academic contexts as well since you won’t often have enough time to explain all the details of what you do.</p> <p>Think of it this way: If you can give an elevator pitch to a 15 year old, you’re probably going to give excellent ones to people you meet at the next conference. If you can come up with an interactive demonstration of your research for a science fair, finding illustrative examples for your next paper will be easy. Communication is a skill and SciComm is communication in hard mode. It can give you great practice along with your research skills for future grant applications, networking and even job hunting.</p> <p>Maybe even more importantly: SciComm forces you to focus on what you’re actually trying to accomplish. What’s the end goal here? How will others interact with your research in 10 to 15 years? If you want to, you can escape these long-term questions as a PhD student, but further along in your career you’ll need to have this kind of plan and be able to convince others that it’s worth working towards. Even if a general audience might not be able to understand the nuances, their questions and reactions can help you find out what topics are important and what others are concerned about. It lets you step outside of your bubble for a bit and refocus on what matters beyond the next paper. I find that’s great motivation for future endeavors.</p> <h3 id="what-can-you-personally-do">What can you personally do?</h3> <p>In lieu of someone asking you to take part in a larger initiative, you can always take part in scicomm events or organize them yourself. The German site <a href="https://www.wissenschaftskommunikation.de">“wissenschaftskommunikation.de“</a> lists many different ideas for formats - unfortunately it seems like this part of the page isn’t translated to English (so you might want to use the translation tool of your choice). Here you can filter by who you want to target and what kind of format you prefer. I recommend taking a look even if some things (like making a video game) might be out of reach for now.</p> <p>Some of my favorites are science slams, science pub crawls and science nights. As you can probably tell, they all have one thing in common: usually they take part in the evening. The important thing to me, though, is the relaxed atmosphere you can create with all three of them and that they’re fairly low effort for organizers and participants. So what are they exactly?</p> <ul> <li> <p>A <strong>science slam</strong> is similar to a poetry slam: around four or five participants get 10 minutes each to speak and then the audience scores the presentations from 1 to 10 (this is usually done by distributing pens and a few sheets of paper, then asking the audience to settle on local scores). Instead of a slam poem, however, the task is to talk about one’s own research in an accessible and entertaining way. Here’s a (German) video of me doing one: <a href="https://www.youtube.com/watch?v=UQ4y5ovvp2Y">https://www.youtube.com/watch?v=UQ4y5ovvp2Y</a> This is really challenging because the crowd is drinking beer, wants a laugh and you’re trying to get across something that can take months or years to understand the details of, but that’s exactly the fun of it. Winning or losing doesn’t really matter since the scoring is pretty unfair anyway, it’s about fun, not about the contest.</p> </li> <li> <p><strong>Science pub crawls</strong> are somewhat similar as they take place in pubs or other places meant for drinking in the evening, but here there are a few different pubs and each scientist or group of scientists get one. Here you can give people a short pitch of your work and then discuss their questions and ideas. This is much more interactive, but still quite hard since the technical equipment is probably limited.</p> </li> <li> <p>A <strong>science night</strong> on the other hand can mean many things, but often they’re organized at universities, so you can keep all your fancy stuff at hand. Often there are many different offerings in the form of talks, experiments, hands-on demonstrations and so on near each other and people can just walk around and see what they’re interested in. It’s a great opportunity to test demonstrators, especially if they’re supposed to be hands on, and you meet a fairly mixed audience with many children. That’s a different kind of challenge altogether and you’ll be tired afterwards, but it’s really rewarding because people are often very interested in the specifics of your research.</p> </li> </ul> <p>Obviously these are only the three I like and think are somewhat feasible in an academic context with limited resources. With more time and money, the sky’s the limit. For example, I recently met someone at a workshop who is working on designing an exhibition in a big German museum for a research cluster that successfully requested a position for museum work in a grant proposal. In the same workshop, someone else curates a newsletter for teens, a third does technology chats with older people over coffee and cake. Or maybe a podcast works better for you? Either way, experimenting with SciComm at least once is in my opinion a great way of practicing presentation skills while making what you do more accessible.</p> <p>What really stuck with me was the introduction to the workshop I mentioned by Prof. Wolfgang Heckl who is the general director of the Deutsches Museum in Munich. He said: „We do science communication because we like people and we want them to take part in scientific advancement“. And why else do science in the first place?</p>]]></content><author><name></name></author><summary type="html"><![CDATA[SciComm can be great, even at a PhD level. Here's why.]]></summary></entry><entry><title type="html">Contextualize Me – The Case for Context in Reinforcement Learning</title><link href="https://theeimer.github.io/blog/2023/contextualize-me-the-case-for-context-in-reinforcement-learning/" rel="alternate" type="text/html" title="Contextualize Me – The Case for Context in Reinforcement Learning"/><published>2023-06-05T08:56:19+00:00</published><updated>2023-06-05T08:56:19+00:00</updated><id>https://theeimer.github.io/blog/2023/contextualize-me--the-case-for-context-in-reinforcement-learning</id><content type="html" xml:base="https://theeimer.github.io/blog/2023/contextualize-me-the-case-for-context-in-reinforcement-learning/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Carolin Benjamins, Theresa Eimer, Frederik Schubert, Aditya Mohan, Sebastian Döhler, André Biedenkapp, Bodo Rosenhahn, Frank Hutter and Marius Lindauer TLDR: We can model and investigate generalization in RL with contextual RL and our benchmark library CARL. In theory, without adding context we cannot achieve optimal performance and in the experiments we saw that using context [&#8230;]]]></summary></entry><entry><title type="html">Hyperparameter Tuning in Reinforcement Learning is Easy, Actually</title><link href="https://theeimer.github.io/blog/2023/hyperparameter-tuning-in-reinforcement-learning-is-easy-actually/" rel="alternate" type="text/html" title="Hyperparameter Tuning in Reinforcement Learning is Easy, Actually"/><published>2023-06-05T08:20:01+00:00</published><updated>2023-06-05T08:20:01+00:00</updated><id>https://theeimer.github.io/blog/2023/hyperparameter-tuning-in-reinforcement-learning-is-easy-actually</id><content type="html" xml:base="https://theeimer.github.io/blog/2023/hyperparameter-tuning-in-reinforcement-learning-is-easy-actually/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Hyperparameter Optimization tools perform well on Reinforcement Learning, outperforming Grid Searches with less than 10% of the budget. If not reported correctly, however, all hyperparameter tuning can heavily skew future comparisons.]]></summary></entry><entry><title type="html">Self-Paced Context Evaluation for Contextual Reinforcement Learning</title><link href="https://theeimer.github.io/blog/2021/self-paced-context-evaluation-for-contextual-reinforcement-learning/" rel="alternate" type="text/html" title="Self-Paced Context Evaluation for Contextual Reinforcement Learning"/><published>2021-07-12T14:37:39+00:00</published><updated>2021-07-12T14:37:39+00:00</updated><id>https://theeimer.github.io/blog/2021/self-paced-context-evaluation-for-contextual-reinforcement-learning</id><content type="html" xml:base="https://theeimer.github.io/blog/2021/self-paced-context-evaluation-for-contextual-reinforcement-learning/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[RL agents, just like humans, often benefit from a difficulty curve in learning [Matiisen et al. 2017, Fuks et al. 2019, Zhang et al. 2020]. Progressing from simple task instances, e.g. walking on flat surfaces or towards goals that are very close to the agent, to more difficult ones lets the agent accomplish much harder [&#8230;]]]></summary></entry><entry><title type="html">DACBench: Benchmarking Dynamic Algorithm Configuration</title><link href="https://theeimer.github.io/blog/2021/dacbench-benchmarking-dynamic-algorithm-configuration/" rel="alternate" type="text/html" title="DACBench: Benchmarking Dynamic Algorithm Configuration"/><published>2021-06-24T13:29:55+00:00</published><updated>2021-06-24T13:29:55+00:00</updated><id>https://theeimer.github.io/blog/2021/dacbench-benchmarking-dynamic-algorithm-configuration</id><content type="html" xml:base="https://theeimer.github.io/blog/2021/dacbench-benchmarking-dynamic-algorithm-configuration/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Dynamic Algorithm Configuration (DAC) has been shown to significantly improve algorithm performance over static or even handcrafted dynamic hyperparameter policies [Biedenkapp et al., 2020]. Most algorithms, however, are not designed with DAC in mind and have to be adapted to be controlled online. This requires a great deal of familiarity with the target algorithm as [&#8230;]]]></summary></entry></feed>