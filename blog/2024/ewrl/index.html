<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>EWRL17 Recap | Theresa Eimer</title> <meta name="author" content="Theresa Eimer"> <meta name="description" content="The EWRL in Toulouse was an awesome event. Here are some of my favorite posters!"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%8D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://theeimer.github.io/blog/2024/ewrl/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "EWRL17 Recap",
      "description": "The EWRL in Toulouse was an awesome event. Here are some of my favorite posters!",
      "published": "November 3, 2024",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Theresa </span>Eimer</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Hi there!</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link text-lowercase" href="/teaching/">Teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>EWRL17 Recap</h1> <p>The EWRL in Toulouse was an awesome event. Here are some of my favorite posters!</p> </d-title> <d-byline></d-byline> <d-article> <h1 id="ewrl17-recap">EWRL17 Recap</h1> <p>This year’s EWRL in Toulouse was a great event: well-organized, interesting program, awesome people. I can only recommend attending the next edition! Here are the most interesting papers I discovered during the poster sessions - if yours isn’t here, it’s most likely due to me getting too caught up chatting or the wine during lunch. I ordered the papers according to the EWRL website, so you shouldn’t take that as an indicator of quality in any way.</p> <h3 id="deterministic-exploration-via-stationary-bellman-error-maximization"><a href="https://openreview.net/forum?id=vYm3GOD8CG" rel="external nofollow noopener" target="_blank">Deterministic Exploration via Stationary Bellman Error Maximization</a></h3> <p>The idea here is to use an exploration strategy that is trained to find really high error states of the actual policy. The fun thing: this policy apparently can discover quite complex Lunar Lander behaviors (e.g. barrel rolls!) this way. It struck me how much more complex the error behavior was compared to the actual solution, I’d be interested to know if it’s actually easier to find complex errors than to do the right thing.</p> <h3 id="maximum-entropy-on-policy-actor-critic-via-entropy-advantage-estimation"><a href="https://openreview.net/forum?id=4osMPTZFBF" rel="external nofollow noopener" target="_blank">Maximum Entropy On-Policy Actor-Critic via Entropy Advantage Estimation</a></h3> <p>The interesting bit here is that we should probably treat the entropy and value advantage estimation differently - if put like that, it seems obvious and the results here support why. Depending on the environment, we want more or less exploration decay and using this method we can control that separately. Basically, we expose a hyperparameter that so far was kept completely static and thus had to serve the same purpose for advantage and entropy estimation. Being able to adjust it is likely going to be an advantage, especially if we can figure out how to do that on the fly.</p> <h3 id="interpretable-and-editable-programmatic-tree-policies-for-reinforcement-learning"><a href="https://openreview.net/forum?id=yDicN3WVZ2" rel="external nofollow noopener" target="_blank">Interpretable and Editable Programmatic Tree Policies for Reinforcement Learning</a></h3> <p>This sounds like such a crazy idea: let’s translate RL policies into Python code! It’s even crazier that the authors figured out how to do this. I’m impressed and really want to try this now.</p> <h3 id="explore-go-leveraging-exploration-for-generalisation-in-deep-reinforcement-learning"><a href="https://openreview.net/forum?id=qteUVvGvFQ" rel="external nofollow noopener" target="_blank">Explore-Go: Leveraging Exploration for Generalisation in Deep Reinforcement Learning</a></h3> <p>Yes, the title refers to Go-Explore, so I asked the authors. This is a much simpler idea, though: just by randomly finding new “starting states” at the beginning of an episode, we can improve generalization. Obviously, there are environments where random actions aren’t helpful, but the point stands: artificially increasing diversity in starting states is important for learning. The fact that we can accomplish it so easily is pretty neat!</p> <h3 id="revisiting-on-policy-deep-reinforcement-learning"><a href="https://openreview.net/forum?id=SYV6AlWh9P" rel="external nofollow noopener" target="_blank">Revisiting On-Policy Deep Reinforcement Learning</a></h3> <p>This paper ablates a few interesting design decisions in SAC and compares it to PPO. What I find most interesting (quite obviously if you know me) is of course the fact that some of these design decisions seem to be somewhat environment-agnostic, at least on MuJoCo, while others might be interesting to adapt, e.g. Momentum or Off-Policy. The difference is also only really visible on the Inverted Double Pendulum, kind of the odd one out environment - meaning that their ON-SAC configuration seems pretty stable within the MuJoCo locomotion domain, but might need adjustment elsewhere. Interesting signals when thinking about connecting algorithm configurations to environment domains.</p> <h3 id="image-based-dataset-representations-for-predicting-learning-performance-in-offline-rl"><a href="https://openreview.net/forum?id=AKU4h6BPG7" rel="external nofollow noopener" target="_blank">Image-Based Dataset Representations for Predicting Learning Performance in Offline RL</a></h3> <p>Another crazy-sounding idea: let’s compress an offline RL dataset into an image for performance prediction! This is a really creative paper and a great idea overall, I recommend taking a look.</p> <h3 id="curricula-for-learning-robust-policies-with-factored-state-representations-in-changing-environments"><a href="https://openreview.net/forum?id=X3i12AKYJn" rel="external nofollow noopener" target="_blank">Curricula for Learning Robust Policies with Factored State Representations in Changing Environments</a></h3> <p>This paper contains quite a few interesting ablations centered around the question of how well an agent generalizes on Frozen Lake variations. It reinforces that even here in this simple environment we don’t really know how to do task scheduling, that different context features have different influences on generalization and that the state representation matters. The ablations are quite thorough and on a simple domain, so I think they’re a nice reference on different factors for zero-shot generalization and task ordering.</p> <h3 id="value-improved-actor-critic-algorithms"><a href="https://openreview.net/forum?id=rXSgsdvpV9" rel="external nofollow noopener" target="_blank">Value Improved Actor Critic Algorithms</a></h3> <p>Why do we use policy improvement operators and not value improvement operators (explicit ones, at least)? This paper tries and there are a few interesting results - most importantly maybe that value improvement operators allow more (or less, if you prefer) greedy improvement than before. This interacts nicely with over- vs underestimation: the danger of being “too pessimistic” in the long run is reduced if we’re very greedy with respect to the pessimistic estimate, which is better than trying to force a more optimistic estimate in my opinion.</p> <h3 id="trust-the-model-where-it-trusts-itself---model-based-actor-critic-with-uncertainty-aware-rollout-adaption"><a href="https://openreview.net/forum?id=VRJaXWiu7j" rel="external nofollow noopener" target="_blank">Trust the Model Where It Trusts Itself - Model-Based Actor-Critic with Uncertainty-Aware Rollout Adaption</a></h3> <p>I was almost convinced this method must have existed beforehand when I came to the poster - this is a compliment! Using the model uncertainty to gauge whether it’s better to use the model to generate samples or collect new experiences feels so much more natural than scheduling the tradeoff without taking the model into account.</p> <h3 id="autorlorg-papers">AutoRL.org Papers</h3> <p>I left out papers I’d seen previously and that have been presented by the AutoRL.org circle - you should still take a look at those, though: <a href="https://openreview.net/forum?id=ZEnbCWsxoL" rel="external nofollow noopener" target="_blank">ARLBench</a>, <a href="https://openreview.net/forum?id=x60cH4KDRv" rel="external nofollow noopener" target="_blank">Relational Structure in Representations</a>, <a href="https://openreview.net/forum?id=LkYaQzGgfL" rel="external nofollow noopener" target="_blank">Joint Optimization of Reward Shaping and Hyperparameters</a> and the wonderfully titled <a href="https://openreview.net/forum?id=zHt4K5zX4P" rel="external nofollow noopener" target="_blank">Dreaming of Many Worlds</a>.</p> <h3 id="bonus">Bonus</h3> <p>I really liked Elise van der Pol’s talk about symmetry. I was vaguely aware of the concepts she talked about before, but apparently it took until now to really connect with me. Super interesting topic and I hope to see more research here!</p> <p>Thanks for reading, hopefully see you again next year!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Theresa Eimer. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. <a href="https://theeimer.github.io/impressum/">Impressum</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>